## Hi there ğŸ‘‹

Hello! I am Yingqing He. Nice to meet you!  
ğŸ‘¨â€ğŸ’»â€ I am currently a PhD student at HKUST. My research focuses on **text-to-video generation** and **multimodal generation**.   
ğŸ“« How to reach me: yhebm@connect.ust.hk  
ğŸ“£ Our lab is hiring engineering-oriented research assistants (RA). If you would like to apply, feel free to reach out with your CV!  

ğŸ§ Recent projects:  
- ğŸŸ **VideoTuna**: An all-in-one codebase for text-to-video applications. [Github](https://github.com/VideoVerses/VideoTuna)
- ğŸ­ **Awesome-LLMs-meet-Multimodal-Generation**: A curated list of papers on LLMs-based multimodal generation (image, video, 3D, and audio). [Github](https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation)
- ğŸ­ **Awesome Diffusion Models in High-Resolution Synthesis** [Github](https://github.com/GuoLanqing/Awesome-High-Resolution-Diffusion)  
- ğŸ­ **[CVPR 2024] Seeing and Hearing**: Open-domain Visual-Audio Generation with Diffusion Latent Aligners. [Github](https://github.com/yzxing87/Seeing-and-Hearing)  
- ğŸ­ **[ECCV 2024] Make a Cheap Scaling**: A Self-Cascade Diffusion Model for Higher-Resolution Adaptation. [Github](https://github.com/GuoLanqing/Self-Cascade)  
